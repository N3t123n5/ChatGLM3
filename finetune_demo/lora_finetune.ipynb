{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:01<00:00,  5.33it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 1603011.65 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 540581.22 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 761307.09 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:01<00:00, 82926.78 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 3085.90 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 2988.48 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/home/n3t123n5/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.7176, 'grad_norm': 2.7301108837127686, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.8816, 'grad_norm': 3.6940393447875977, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 4.5443, 'grad_norm': 3.547790765762329, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1578, 'grad_norm': 4.552702903747559, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 4.0861, 'grad_norm': 3.7000184059143066, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}\n",
      "{'loss': 4.0574, 'grad_norm': 3.8659873008728027, 'learning_rate': 4.9e-05, 'epoch': 0.0}\n",
      "{'loss': 3.915, 'grad_norm': 4.078589916229248, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6824, 'grad_norm': 4.282929420471191, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9248, 'grad_norm': 4.090704917907715, 'learning_rate': 4.85e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7914, 'grad_norm': 3.8826286792755127, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6268, 'grad_norm': 3.961845636367798, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7553, 'grad_norm': 4.408216953277588, 'learning_rate': 4.8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6578, 'grad_norm': 5.663897514343262, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6932, 'grad_norm': 5.594644069671631, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6377, 'grad_norm': 4.908491611480713, 'learning_rate': 4.75e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6437, 'grad_norm': 4.517873764038086, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5229, 'grad_norm': 6.046231269836426, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5562, 'grad_norm': 5.081092357635498, 'learning_rate': 4.7e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6453, 'grad_norm': 6.195070743560791, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.583, 'grad_norm': 5.529261112213135, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5389, 'grad_norm': 6.417802333831787, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.0}\n",
      "{'loss': 3.524, 'grad_norm': 6.22446346282959, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8398, 'grad_norm': 5.076383590698242, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6187, 'grad_norm': 7.284204006195068, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4592, 'grad_norm': 6.363934516906738, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5332, 'grad_norm': 6.053835391998291, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6324, 'grad_norm': 5.810608863830566, 'learning_rate': 4.55e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5998, 'grad_norm': 6.08749532699585, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5766, 'grad_norm': 7.890063285827637, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5709, 'grad_norm': 5.449385643005371, 'learning_rate': 4.5e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5934, 'grad_norm': 6.54104471206665, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7004, 'grad_norm': 6.304164886474609, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4553, 'grad_norm': 6.332497596740723, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5422, 'grad_norm': 5.945121765136719, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4088, 'grad_norm': 7.327879428863525, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5232, 'grad_norm': 6.268461227416992, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5186, 'grad_norm': 7.86282205581665, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3943, 'grad_norm': 7.926567077636719, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4857, 'grad_norm': 7.433795928955078, 'learning_rate': 4.35e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4518, 'grad_norm': 6.911885738372803, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4, 'grad_norm': 6.743048191070557, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5207, 'grad_norm': 7.45697021484375, 'learning_rate': 4.3e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6238, 'grad_norm': 6.558588981628418, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4818, 'grad_norm': 5.932514667510986, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6107, 'grad_norm': 6.222145080566406, 'learning_rate': 4.25e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3857, 'grad_norm': 8.12188720703125, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4131, 'grad_norm': 7.160132884979248, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.433, 'grad_norm': 6.707749366760254, 'learning_rate': 4.2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3199, 'grad_norm': 6.745841026306152, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4809, 'grad_norm': 7.563328742980957, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.01}\n",
      " 17%|██████▋                                 | 500/3000 [02:03<10:11,  4.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:01<00:10,  1.04it/s]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:03<00:12,  1.26s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:05<00:13,  1.53s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:08<00:14,  1.84s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:10<00:13,  1.92s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:24<00:35,  5.89s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:26<00:23,  4.67s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:28<00:15,  3.80s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:30<00:09,  3.23s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:31<00:05,  2.78s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:33<00:02,  2.46s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 13/13 [00:35<00:00,  2.27s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.394 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 29.444581999999997, 'eval_rouge-2': 6.257174, 'eval_rouge-l': 23.594642, 'eval_bleu-4': 0.031108126481668292, 'eval_runtime': 50.4444, 'eval_samples_per_second': 0.991, 'eval_steps_per_second': 0.258, 'epoch': 0.01}\n",
      " 17%|██████▋                                 | 500/3000 [02:53<10:11,  4.09it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:36<00:00,  2.27s/it]\u001b[A\n",
      "{'loss': 3.4898, 'grad_norm': 7.8029985427856445, 'learning_rate': 4.15e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5717, 'grad_norm': 8.400986671447754, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3693, 'grad_norm': 6.67218542098999, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5885, 'grad_norm': 8.25823974609375, 'learning_rate': 4.1e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5842, 'grad_norm': 6.896370887756348, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4641, 'grad_norm': 7.268834114074707, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5738, 'grad_norm': 7.643599510192871, 'learning_rate': 4.05e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4779, 'grad_norm': 7.337422847747803, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3738, 'grad_norm': 6.654488563537598, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.532, 'grad_norm': 8.110974311828613, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3932, 'grad_norm': 7.395680904388428, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3742, 'grad_norm': 7.335953712463379, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5168, 'grad_norm': 7.803590297698975, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5307, 'grad_norm': 8.64559555053711, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4318, 'grad_norm': 9.06615161895752, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2098, 'grad_norm': 6.871189117431641, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3877, 'grad_norm': 7.4503560066223145, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4281, 'grad_norm': 6.848163604736328, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4637, 'grad_norm': 7.2517571449279785, 'learning_rate': 3.85e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4002, 'grad_norm': 7.9241838455200195, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5186, 'grad_norm': 7.383617401123047, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4586, 'grad_norm': 7.870441436767578, 'learning_rate': 3.8e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3664, 'grad_norm': 12.896255493164062, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1881, 'grad_norm': 6.7708306312561035, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3805, 'grad_norm': 8.660656929016113, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4906, 'grad_norm': 6.896272659301758, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.373, 'grad_norm': 7.491537570953369, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.01}\n",
      "{'loss': 3.515, 'grad_norm': 7.976644992828369, 'learning_rate': 3.7e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4041, 'grad_norm': 8.042344093322754, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4092, 'grad_norm': 7.2780914306640625, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5455, 'grad_norm': 7.960363388061523, 'learning_rate': 3.65e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6627, 'grad_norm': 7.70579719543457, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3037, 'grad_norm': 7.285727500915527, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5475, 'grad_norm': 6.457652568817139, 'learning_rate': 3.6e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4873, 'grad_norm': 7.55821418762207, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5799, 'grad_norm': 7.09037446975708, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3039, 'grad_norm': 7.84348726272583, 'learning_rate': 3.55e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3566, 'grad_norm': 9.17190933227539, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1701, 'grad_norm': 7.44560432434082, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4977, 'grad_norm': 8.426700592041016, 'learning_rate': 3.5e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2924, 'grad_norm': 8.707014083862305, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3262, 'grad_norm': 7.883038520812988, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5273, 'grad_norm': 8.208635330200195, 'learning_rate': 3.45e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3947, 'grad_norm': 7.001204013824463, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.285, 'grad_norm': 7.526392936706543, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4309, 'grad_norm': 10.231633186340332, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3855, 'grad_norm': 7.7630720138549805, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3455, 'grad_norm': 8.719307899475098, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5908, 'grad_norm': 8.687384605407715, 'learning_rate': 3.35e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3664, 'grad_norm': 8.509410858154297, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.02}\n",
      " 33%|█████████████                          | 1000/3000 [04:57<09:29,  3.51it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:01<00:10,  1.07it/s]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:16<01:04,  6.44s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:18<00:43,  4.81s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:20<00:31,  3.91s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:22<00:23,  3.41s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:25<00:19,  3.28s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:40<00:33,  6.75s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:42<00:21,  5.34s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:43<00:12,  4.18s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:45<00:06,  3.38s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:46<00:02,  2.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.515310000000003, 'eval_rouge-2': 6.942985999999999, 'eval_rouge-l': 24.749287999999996, 'eval_bleu-4': 0.03063849601519664, 'eval_runtime': 50.2066, 'eval_samples_per_second': 0.996, 'eval_steps_per_second': 0.259, 'epoch': 0.02}\n",
      " 33%|█████████████                          | 1000/3000 [05:47<09:29,  3.51it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:48<00:00,  2.39s/it]\u001b[A\n",
      "{'loss': 3.2537, 'grad_norm': 7.8156914710998535, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2398, 'grad_norm': 7.815760135650635, 'learning_rate': 3.3e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4914, 'grad_norm': 8.869853973388672, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4652, 'grad_norm': 8.54748249053955, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5746, 'grad_norm': 8.832815170288086, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.408, 'grad_norm': 8.1636323928833, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3994, 'grad_norm': 7.710549354553223, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3818, 'grad_norm': 7.9419264793396, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5562, 'grad_norm': 10.146836280822754, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3291, 'grad_norm': 8.159811019897461, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5693, 'grad_norm': 9.595257759094238, 'learning_rate': 3.15e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5422, 'grad_norm': 8.849584579467773, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4102, 'grad_norm': 7.709377288818359, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4172, 'grad_norm': 8.210613250732422, 'learning_rate': 3.1e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2824, 'grad_norm': 8.6553316116333, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.308, 'grad_norm': 9.433565139770508, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1809, 'grad_norm': 7.306060314178467, 'learning_rate': 3.05e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4936, 'grad_norm': 9.357596397399902, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4119, 'grad_norm': 9.398211479187012, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4215, 'grad_norm': 8.499898910522461, 'learning_rate': 3e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4193, 'grad_norm': 9.600312232971191, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2877, 'grad_norm': 9.19770622253418, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2406, 'grad_norm': 9.427164077758789, 'learning_rate': 2.95e-05, 'epoch': 0.02}\n",
      "{'loss': 3.492, 'grad_norm': 8.682910919189453, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3201, 'grad_norm': 8.86834716796875, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4271, 'grad_norm': 8.079267501831055, 'learning_rate': 2.9e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2455, 'grad_norm': 8.907357215881348, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5113, 'grad_norm': 8.960707664489746, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3682, 'grad_norm': 8.603240013122559, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5404, 'grad_norm': 7.959038257598877, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.326, 'grad_norm': 8.762590408325195, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4725, 'grad_norm': 9.411287307739258, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5283, 'grad_norm': 9.938316345214844, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3879, 'grad_norm': 8.834932327270508, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3432, 'grad_norm': 8.23620319366455, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.1076, 'grad_norm': 8.090893745422363, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3355, 'grad_norm': 9.23486042022705, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3045, 'grad_norm': 9.590143203735352, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2635, 'grad_norm': 11.114275932312012, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3205, 'grad_norm': 8.665912628173828, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5281, 'grad_norm': 8.777676582336426, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2766, 'grad_norm': 10.151689529418945, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.2287, 'grad_norm': 8.063131332397461, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6723, 'grad_norm': 9.652198791503906, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.03}\n",
      "{'loss': 3.193, 'grad_norm': 8.143725395202637, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1469, 'grad_norm': 9.185768127441406, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6033, 'grad_norm': 10.311376571655273, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3629, 'grad_norm': 8.270244598388672, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3738, 'grad_norm': 9.894721984863281, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2656, 'grad_norm': 8.812281608581543, 'learning_rate': 2.5e-05, 'epoch': 0.03}\n",
      " 50%|███████████████████▌                   | 1500/3000 [07:48<05:34,  4.48it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:14<01:17,  7.03s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:16<00:50,  5.09s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:18<00:34,  3.81s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:20<00:26,  3.26s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:34<00:48,  6.88s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:36<00:32,  5.42s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:39<00:22,  4.40s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:41<00:14,  3.73s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:42<00:09,  3.08s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:44<00:05,  2.71s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:46<00:02,  2.43s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.269059999999996, 'eval_rouge-2': 6.1988780000000006, 'eval_rouge-l': 23.586631999999998, 'eval_bleu-4': 0.028276343338273495, 'eval_runtime': 74.3744, 'eval_samples_per_second': 0.672, 'eval_steps_per_second': 0.175, 'epoch': 0.03}\n",
      " 50%|███████████████████▌                   | 1500/3000 [09:03<05:34,  4.48it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:00<00:00,  5.73s/it]\u001b[A\n",
      "{'loss': 3.241, 'grad_norm': 7.634515285491943, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5525, 'grad_norm': 7.980646133422852, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4523, 'grad_norm': 7.9525980949401855, 'learning_rate': 2.45e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6377, 'grad_norm': 10.918983459472656, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.448, 'grad_norm': 9.350662231445312, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3305, 'grad_norm': 10.492191314697266, 'learning_rate': 2.4e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3357, 'grad_norm': 8.541833877563477, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.182, 'grad_norm': 9.136805534362793, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.03}\n",
      "{'loss': 3.451, 'grad_norm': 9.662114143371582, 'learning_rate': 2.35e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4807, 'grad_norm': 8.00162410736084, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2619, 'grad_norm': 7.162015914916992, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1555, 'grad_norm': 9.146615028381348, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1842, 'grad_norm': 8.886709213256836, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4025, 'grad_norm': 9.2765531539917, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.03}\n",
      "{'loss': 3.348, 'grad_norm': 9.465653419494629, 'learning_rate': 2.25e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3785, 'grad_norm': 10.092035293579102, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2758, 'grad_norm': 7.674713134765625, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3613, 'grad_norm': 7.739775657653809, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4197, 'grad_norm': 8.702583312988281, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4152, 'grad_norm': 8.366575241088867, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4848, 'grad_norm': 9.320755958557129, 'learning_rate': 2.15e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4584, 'grad_norm': 8.592061996459961, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3451, 'grad_norm': 9.789348602294922, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1146, 'grad_norm': 9.588793754577637, 'learning_rate': 2.1e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3646, 'grad_norm': 8.86318302154541, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4256, 'grad_norm': 8.624138832092285, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4613, 'grad_norm': 10.360724449157715, 'learning_rate': 2.05e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2775, 'grad_norm': 9.826136589050293, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1914, 'grad_norm': 8.84506893157959, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1686, 'grad_norm': 9.888090133666992, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4023, 'grad_norm': 8.91452407836914, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3447, 'grad_norm': 10.923837661743164, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2758, 'grad_norm': 12.056375503540039, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.398, 'grad_norm': 10.593069076538086, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4746, 'grad_norm': 9.98263168334961, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2898, 'grad_norm': 10.63159465789795, 'learning_rate': 1.9e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5246, 'grad_norm': 9.028886795043945, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4709, 'grad_norm': 10.453153610229492, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.384, 'grad_norm': 8.195974349975586, 'learning_rate': 1.85e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2082, 'grad_norm': 9.16545581817627, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3961, 'grad_norm': 10.670598983764648, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3189, 'grad_norm': 9.460145950317383, 'learning_rate': 1.8e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4652, 'grad_norm': 10.410999298095703, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4369, 'grad_norm': 7.931452751159668, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.03}\n",
      "{'loss': 3.1475, 'grad_norm': 9.03185749053955, 'learning_rate': 1.75e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2848, 'grad_norm': 9.610726356506348, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3289, 'grad_norm': 8.15498161315918, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4092, 'grad_norm': 9.69137954711914, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3568, 'grad_norm': 9.844435691833496, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.274, 'grad_norm': 9.403298377990723, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.03}\n",
      " 67%|██████████████████████████             | 2000/3000 [11:04<04:12,  3.95it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:02<00:12,  1.13s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:03<00:13,  1.31s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:05<00:13,  1.54s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:07<00:14,  1.77s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:09<00:12,  1.77s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:12<00:13,  2.18s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:14<00:10,  2.10s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:16<00:08,  2.01s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:18<00:05,  1.87s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:20<00:03,  1.89s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:21<00:01,  1.78s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.491836000000003, 'eval_rouge-2': 6.700698000000001, 'eval_rouge-l': 24.928469999999997, 'eval_bleu-4': 0.030329345016613073, 'eval_runtime': 25.1324, 'eval_samples_per_second': 1.989, 'eval_steps_per_second': 0.517, 'epoch': 0.03}\n",
      " 67%|██████████████████████████             | 2000/3000 [11:29<04:12,  3.95it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:23<00:00,  1.72s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "{'loss': 3.4814, 'grad_norm': 8.440116882324219, 'learning_rate': 1.65e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2662, 'grad_norm': 9.379354476928711, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3018, 'grad_norm': 9.007017135620117, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4391, 'grad_norm': 11.148209571838379, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5154, 'grad_norm': 9.205145835876465, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.627, 'grad_norm': 10.222354888916016, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3941, 'grad_norm': 9.003594398498535, 'learning_rate': 1.55e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2615, 'grad_norm': 7.726950168609619, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4754, 'grad_norm': 8.975449562072754, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.1549, 'grad_norm': 11.9136962890625, 'learning_rate': 1.5e-05, 'epoch': 0.04}\n",
      "{'loss': 3.1764, 'grad_norm': 8.97246265411377, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.04}\n",
      "{'loss': 3.332, 'grad_norm': 10.608307838439941, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4482, 'grad_norm': 8.929343223571777, 'learning_rate': 1.45e-05, 'epoch': 0.04}\n",
      "{'loss': 3.1504, 'grad_norm': 10.501776695251465, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3455, 'grad_norm': 11.407543182373047, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4381, 'grad_norm': 8.943217277526855, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5156, 'grad_norm': 9.689766883850098, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3682, 'grad_norm': 10.411611557006836, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4574, 'grad_norm': 9.428943634033203, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3295, 'grad_norm': 9.35375690460205, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2395, 'grad_norm': 9.574884414672852, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2885, 'grad_norm': 9.162651062011719, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2547, 'grad_norm': 9.17687702178955, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6426, 'grad_norm': 10.977346420288086, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2648, 'grad_norm': 9.029768943786621, 'learning_rate': 1.25e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4123, 'grad_norm': 9.778824806213379, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.1918, 'grad_norm': 9.863383293151855, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3031, 'grad_norm': 11.448969841003418, 'learning_rate': 1.2e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2842, 'grad_norm': 9.620270729064941, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.1756, 'grad_norm': 9.379923820495605, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3369, 'grad_norm': 11.488072395324707, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2217, 'grad_norm': 8.525694847106934, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4211, 'grad_norm': 9.415797233581543, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2787, 'grad_norm': 8.090798377990723, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3043, 'grad_norm': 8.949644088745117, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4578, 'grad_norm': 8.530191421508789, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3629, 'grad_norm': 8.807326316833496, 'learning_rate': 1.05e-05, 'epoch': 0.04}\n",
      "{'loss': 3.1854, 'grad_norm': 9.268948554992676, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.227, 'grad_norm': 10.767414093017578, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4422, 'grad_norm': 9.548675537109375, 'learning_rate': 1e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2014, 'grad_norm': 12.040035247802734, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.04}\n",
      "{'loss': 3.0947, 'grad_norm': 9.186870574951172, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.04}\n",
      "{'loss': 3.1355, 'grad_norm': 12.654911994934082, 'learning_rate': 9.5e-06, 'epoch': 0.04}\n",
      "{'loss': 3.366, 'grad_norm': 9.2201509475708, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.04}\n",
      "{'loss': 3.248, 'grad_norm': 9.443873405456543, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.04}\n",
      "{'loss': 3.4088, 'grad_norm': 9.843236923217773, 'learning_rate': 9e-06, 'epoch': 0.04}\n",
      "{'loss': 3.3135, 'grad_norm': 10.662832260131836, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.04}\n",
      "{'loss': 3.2877, 'grad_norm': 17.47159194946289, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.04}\n",
      "{'loss': 3.4684, 'grad_norm': 10.0890474319458, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 3.2811, 'grad_norm': 9.297119140625, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.04}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [13:31<02:15,  3.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:14<01:17,  7.05s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:15<00:47,  4.73s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:17<00:33,  3.67s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:19<00:25,  3.17s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:21<00:19,  2.79s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:35<00:38,  6.46s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:37<00:25,  5.08s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:40<00:16,  4.22s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:54<00:21,  7.24s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:56<00:11,  5.58s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:58<00:04,  4.65s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.398644, 'eval_rouge-2': 6.938624, 'eval_rouge-l': 23.784438000000005, 'eval_bleu-4': 0.030823191343656742, 'eval_runtime': 62.1885, 'eval_samples_per_second': 0.804, 'eval_steps_per_second': 0.209, 'epoch': 0.04}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [14:33<02:15,  3.69it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:00<00:00,  3.69s/it]\u001b[A\n",
      "{'loss': 3.2109, 'grad_norm': 9.993685722351074, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.04}\n",
      "{'loss': 3.1836, 'grad_norm': 9.731045722961426, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 3.4258, 'grad_norm': 9.446788787841797, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.04}\n",
      "{'loss': 3.3035, 'grad_norm': 9.883879661560059, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.04}\n",
      "{'loss': 3.2898, 'grad_norm': 11.959521293640137, 'learning_rate': 7.5e-06, 'epoch': 0.04}\n",
      "{'loss': 3.2355, 'grad_norm': 10.276638984680176, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.04}\n",
      "{'loss': 3.326, 'grad_norm': 11.50603199005127, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.04}\n",
      "{'loss': 3.3252, 'grad_norm': 9.972771644592285, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 3.473, 'grad_norm': 10.596712112426758, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3369, 'grad_norm': 9.321480751037598, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.5121, 'grad_norm': 8.479673385620117, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2402, 'grad_norm': 9.05765438079834, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 3.4037, 'grad_norm': 8.712411880493164, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.36, 'grad_norm': 9.433768272399902, 'learning_rate': 6e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3656, 'grad_norm': 8.86423110961914, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3139, 'grad_norm': 12.767179489135742, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.1996, 'grad_norm': 9.995779991149902, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2719, 'grad_norm': 9.348297119140625, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2232, 'grad_norm': 10.866700172424316, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3125, 'grad_norm': 9.191521644592285, 'learning_rate': 5e-06, 'epoch': 0.05}\n",
      "{'loss': 3.134, 'grad_norm': 9.899762153625488, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3336, 'grad_norm': 9.590473175048828, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.5057, 'grad_norm': 9.473546981811523, 'learning_rate': 4.5e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3832, 'grad_norm': 10.232597351074219, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2635, 'grad_norm': 8.500922203063965, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3381, 'grad_norm': 10.110013008117676, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3713, 'grad_norm': 9.840497970581055, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2148, 'grad_norm': 8.916417121887207, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3719, 'grad_norm': 10.186691284179688, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.05}\n",
      "{'loss': 3.302, 'grad_norm': 8.340965270996094, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2531, 'grad_norm': 9.667926788330078, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2717, 'grad_norm': 10.210844993591309, 'learning_rate': 3e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2375, 'grad_norm': 9.364901542663574, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.05}\n",
      "{'loss': 3.1662, 'grad_norm': 10.665855407714844, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2906, 'grad_norm': 9.868402481079102, 'learning_rate': 2.5e-06, 'epoch': 0.05}\n",
      "{'loss': 3.333, 'grad_norm': 10.493739128112793, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2869, 'grad_norm': 12.644397735595703, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2725, 'grad_norm': 9.874338150024414, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2811, 'grad_norm': 9.458573341369629, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.05}\n",
      "{'loss': 3.1225, 'grad_norm': 8.955205917358398, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3271, 'grad_norm': 8.090843200683594, 'learning_rate': 1.5e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3648, 'grad_norm': 10.464899063110352, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.05}\n",
      "{'loss': 3.4238, 'grad_norm': 9.527956008911133, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3008, 'grad_norm': 11.023272514343262, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2506, 'grad_norm': 9.460820198059082, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.05}\n",
      "{'loss': 3.2172, 'grad_norm': 9.060050964355469, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.05}\n",
      "{'loss': 3.4301, 'grad_norm': 11.894376754760742, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.05}\n",
      "{'loss': 3.3178, 'grad_norm': 8.948634147644043, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.05}\n",
      "{'loss': 3.3768, 'grad_norm': 11.667614936828613, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.05}\n",
      "{'loss': 3.3576, 'grad_norm': 9.572566986083984, 'learning_rate': 0.0, 'epoch': 0.05}\n",
      "100%|███████████████████████████████████████| 3000/3000 [16:33<00:00,  4.50it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:02<00:11,  1.08s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:04<00:14,  1.45s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:05<00:13,  1.54s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:07<00:13,  1.63s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:21<00:40,  5.78s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:35<00:50,  8.48s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:37<00:32,  6.42s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:39<00:20,  5.02s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:53<00:23,  7.79s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:55<00:11,  5.99s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:57<00:04,  4.79s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.189176000000003, 'eval_rouge-2': 7.114307999999999, 'eval_rouge-l': 23.875558, 'eval_bleu-4': 0.034430076516767036, 'eval_runtime': 62.1809, 'eval_samples_per_second': 0.804, 'eval_steps_per_second': 0.209, 'epoch': 0.05}\n",
      "100%|███████████████████████████████████████| 3000/3000 [17:36<00:00,  4.50it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:59<00:00,  3.96s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1056.0257, 'train_samples_per_second': 5.682, 'train_steps_per_second': 2.841, 'train_loss': 3.4140013020833333, 'epoch': 0.05}\n",
      "100%|███████████████████████████████████████| 3000/3000 [17:36<00:00,  2.84it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 4\n",
      "100%|█████████████████████████████████████████| 268/268 [17:05<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 /home/n3t123n5/miniconda3/envs/transformer/bin/python finetune_hf.py  data/AdvertiseGen_fix  /home/n3t123n5/Data/Models/LLM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-2000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:01<00:00,  4.50it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "这款连衣裙的版型是套头式的，版型很显瘦，穿着很舒适。不规则的百褶裙摆，很显高挑，搭配网纱材质，更具有性感魅力。胸部的木耳边装饰，很显气质。裙身压褶的抽褶设计，很显瘦。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0  /home/n3t123n5/miniconda3/envs/transformer/bin/python inference_hf.py output/checkpoint-2000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed1256-1fc0-4b7b-8feb-e74ff5c67b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
